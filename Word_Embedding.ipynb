{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c3e68e-f80c-4422-bf2b-58f5ac56b3ad",
   "metadata": {},
   "source": [
    "### Word Embedding Techniques\n",
    "Word Embeddings are the texts converted into numbers.\n",
    "A Word Embedding format generally tries to map a word using a dictionary to a vector. Let us break this sentence down into finer details to have a clear view.\n",
    "\n",
    "\"Word Embeddings are Words converted into numbers\"\n",
    "\n",
    "A dictionary is the list of all unique words in the sentence. So, a dictionary may look like – ['Word','Embeddings','are','words','Converted','into','numbers']\n",
    "\n",
    "A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of \"numbers\" in this format according to the above dictionary is [0,0,0,0,0,1] and \"converted\" is[0,0,0,1,0,0]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a2ce16-e44e-4dc0-a178-aaddaa19f4d9",
   "metadata": {},
   "source": [
    "Types of Word Embeddings\n",
    "\n",
    "1.Frequency based Embedding\n",
    "\n",
    "2.Prediction based Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f52f8d-ab45-45e1-8299-1143e6679997",
   "metadata": {},
   "source": [
    "Frequency based Embedding\n",
    "\n",
    "Count Vector\n",
    "\n",
    "TF-IDF Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6633974b-02ed-484d-be7f-9aa26ab5d3c6",
   "metadata": {},
   "source": [
    "Count Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b40ee8-3e88-4f15-9f7c-d1ee6fff721e",
   "metadata": {},
   "source": [
    "Consider a Corpus C of D documents {d1,d2…..dD} and N unique tokens extracted out of the corpus C. The\n",
    "N tokens will form our dictionary and the size of the Count Vector matrix M will be given by D X N. \n",
    "Each row in the matrix M contains the frequency of tokens in document D(i).\n",
    "\n",
    "Let us understand this using a simple example.\n",
    "\n",
    "D1: He is lazy boy. She is also lazy.\n",
    "\n",
    "D2: Neeta is lazy person.\n",
    "\n",
    "The dictionary created may be a list of unique tokens(words) in the corpus =\n",
    "['He','is','She','lazy','boy','also','Neeta','person']\n",
    "Here, D=2, N=8\n",
    "The count matrix M of size 2 X 8 will be represented as –\n",
    "\n",
    "\n",
    "D1 1 2 1 2 1 1 0 0\n",
    "\n",
    "D2 0 1 0 1 0 0 1 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cf7be21-26a4-4655-b3be-7473d7fe25ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['also', 'boy', 'he', 'is', 'lazy', 'neeta', 'person', 'she']\n",
      "Encoded Document is:\n",
      "[[1 1 1 2 2 0 0 1]\n",
      " [0 0 0 1 1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "#Count Vectorizer\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "document = [\"He is lazy boy. She is also lazy.\",\n",
    "            \"Neeta is lazy person.\"]\n",
    "\n",
    "# Create a Vectorizer Object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "vectorizer.fit(document)\n",
    "\n",
    "# Printing the identified Unique words along with their indices\n",
    "print(\"Vocabulary: \", sorted(vectorizer.vocabulary_))\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(document)\n",
    "\n",
    "# Summarizing the Encoded Texts\n",
    "print(\"Encoded Document is:\")\n",
    "print(vector.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52945899-37af-40b9-954d-e68fb1195c21",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### TF-IDF Vectorizer\n",
    "It is different to the count vectorization in the sense that it takes into account not just the occurrence of a word in a single document but in the entire corpus. \n",
    "TF-IDF works by penalising common words like ('the','a','is') by assigning them lower weights while giving importance\n",
    "to significant words in a particular document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384eac4f-18ae-40da-ab10-d53039288be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "{'boy': 1.4054651081081644, 'lazy': 1.0, 'neeta': 1.4054651081081644, 'person': 1.4054651081081644}\n",
      "TF-IDF values:\n",
      "                 boy      lazy     neeta    person\n",
      "Document 1  0.574962  0.818180  0.000000  0.000000\n",
      "Document 2  0.000000  0.449436  0.631667  0.631667\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "d1=\"He is lazy boy. She is also lazy.\"\n",
    "\n",
    "d2=\"Neeta is lazy person.\"\n",
    "\n",
    "doc_corpus=[d1,d2]\n",
    "\n",
    "# create object\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "\n",
    "# Get TF-IDF values\n",
    "tfidf_matrix = tfidf.fit_transform(doc_corpus)\n",
    "\n",
    "\n",
    "# get idf values\n",
    "print('\\nidf values:')\n",
    "dic = dict(zip(tfidf.get_feature_names_out(), tfidf.idf_))\n",
    "print(dic)\n",
    "\n",
    "\n",
    "# Create a dataframe for the TF-IDF values\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "df.index = [\"Document 1\", \"Document 2\"]\n",
    "\n",
    "# Print the TF-IDF table\n",
    "print(\"TF-IDF values:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89dbecec-f6e0-4e4c-9f84-dab994f25679",
   "metadata": {},
   "source": [
    "Prediction based Embedding\n",
    "\n",
    "CBOW\n",
    "\n",
    "Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "551c9df3-6195-43a2-b3a5-86ede4edcd37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.11/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.11/site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.11/site-packages (4.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in /opt/conda/lib/python3.11/site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b96d89bd-10bf-4825-b066-059c7b72cb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd72b111-3bf6-4a01-a018-e9fa8df723a6",
   "metadata": {},
   "source": [
    "#### CBOW\n",
    "CBOW model predicts the current word given context words within a specific window. The input layer contains the context words and the output layer contains the current word. The hidden layer contains the number of dimensions in which we want to represent the current word present at the output layer. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecbcaad-28b3-44d1-9c81-8bb031d2ef32",
   "metadata": {},
   "source": [
    "#### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c548f357-562d-441c-b354-1cd03eaab3ef",
   "metadata": {},
   "source": [
    "Skip – gram follows the same topology as of CBOW. It just flips CBOW’s architecture on its head. \n",
    "The aim of skip-gram is to predict the context given a word."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
