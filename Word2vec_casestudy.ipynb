{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c54780-18c2-4570-b320-345c97858b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from gensim) (1.11.3)\n",
      "Collecting smart_open>=1.8.1 (from gensim)\n",
      "  Using cached smart_open-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting wrapt (from smart_open>=1.8.1->gensim)\n",
      "  Using cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (9.0 kB)\n",
      "Using cached gensim-4.4.0-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.8 MB)\n",
      "Using cached smart_open-7.5.0-py3-none-any.whl (63 kB)\n",
      "Using cached wrapt-2.0.1-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (114 kB)\n",
      "Installing collected packages: wrapt, smart_open, gensim\n",
      "Successfully installed gensim-4.4.0 smart_open-7.5.0 wrapt-2.0.1\n",
      "Collecting nltk\n",
      "  Using cached nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from nltk) (4.66.4)\n",
      "Using cached nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.2 regex-2025.11.3\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79b3f82f-d9fc-4a2d-8625-1d37e5d3388d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from tensorflow) (24.1)\n",
      "Collecting protobuf>=5.28.0 (from tensorflow)\n",
      "  Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.11/site-packages (from tensorflow) (70.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.2.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (2.0.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.10.0 (from tensorflow)\n",
      "  Using cached keras-3.12.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow) (3.11.0)\n",
      "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Collecting rich (from keras>=3.10.0->tensorflow)\n",
      "  Using cached rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.10.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.10.0->tensorflow)\n",
      "  Using cached optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.6.2)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached markdown-3.10-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.11/site-packages (from tensorboard~=2.20.0->tensorflow) (10.4.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.20.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (2.1.5)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich->keras>=3.10.0->tensorflow) (2.18.0)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (620.6 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.76.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.6 MB)\n",
      "Using cached keras-3.12.0-py3-none-any.whl (1.5 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached ml_dtypes-0.5.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Using cached tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-3.2.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached markdown-3.10-py3-none-any.whl (107 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Using cached optree-0.18.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (400 kB)\n",
      "Using cached rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt_einsum, ml_dtypes, mdurl, markdown, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp-pipeline-spec 0.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 6.33.1 which is incompatible.\n",
      "kfp 2.7.0 requires protobuf<5,>=4.21.1, but you have protobuf 6.33.1 which is incompatible.\n",
      "google-api-core 2.19.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5, but you have protobuf 6.33.1 which is incompatible.\n",
      "googleapis-common-protos 1.63.2 requires protobuf!=3.20.0,!=3.20.1,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.20.2, but you have protobuf 6.33.1 which is incompatible.\n",
      "proto-plus 1.24.0 requires protobuf<6.0.0dev,>=3.19.0, but you have protobuf 6.33.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.76.0 keras-3.12.0 libclang-18.1.1 markdown-3.10 markdown-it-py-4.0.0 mdurl-0.1.2 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.18.0 protobuf-6.33.1 rich-14.2.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.2.0 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8028274a-2578-4ad7-b7a6-da1e69b6cbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Using cached tensorflow_datasets-4.9.9-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (2.3.1)\n",
      "Collecting array_record>=0.5.0 (from tensorflow_datasets)\n",
      "  Using cached array_record-0.8.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Using cached dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting etils>=1.9.1 (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Using cached etils-1.13.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting immutabledict (from tensorflow_datasets)\n",
      "  Using cached immutabledict-4.2.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (1.26.4)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Using cached promise-2.3-py3-none-any.whl\n",
      "Requirement already satisfied: protobuf>=3.20 in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (6.33.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (6.0.0)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (16.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (2.32.3)\n",
      "Collecting simple_parsing (from tensorflow_datasets)\n",
      "  Using cached simple_parsing-0.1.7-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Using cached tensorflow_metadata-1.17.2-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (3.2.0)\n",
      "Requirement already satisfied: toml in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (4.66.4)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.11/site-packages (from tensorflow_datasets) (2.0.1)\n",
      "Collecting einops (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2024.6.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.4.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.11/site-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.19.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.19.0->tensorflow_datasets) (2024.6.2)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /opt/conda/lib/python3.11/site-packages (from dm-tree->tensorflow_datasets) (23.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /opt/conda/lib/python3.11/site-packages (from simple_parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /opt/conda/lib/python3.11/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.63.2)\n",
      "Collecting protobuf>=3.20 (from tensorflow_datasets)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Using cached tensorflow_datasets-4.9.9-py3-none-any.whl (5.3 MB)\n",
      "Using cached array_record-0.8.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "Using cached etils-1.13.0-py3-none-any.whl (170 kB)\n",
      "Using cached dm_tree-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (152 kB)\n",
      "Using cached immutabledict-4.2.2-py3-none-any.whl (4.7 kB)\n",
      "Using cached simple_parsing-0.1.7-py3-none-any.whl (112 kB)\n",
      "Using cached tensorflow_metadata-1.17.2-py3-none-any.whl (31 kB)\n",
      "Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: simple_parsing, protobuf, promise, immutabledict, etils, einops, dm-tree, tensorflow-metadata, array_record, tensorflow_datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 6.33.1\n",
      "    Uninstalling protobuf-6.33.1:\n",
      "      Successfully uninstalled protobuf-6.33.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp-pipeline-spec 0.3.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.29.5 which is incompatible.\n",
      "kfp 2.7.0 requires protobuf<5,>=4.21.1, but you have protobuf 5.29.5 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed array_record-0.8.3 dm-tree-0.1.9 einops-0.8.1 etils-1.13.0 immutabledict-4.2.2 promise-2.3 protobuf-5.29.5 simple_parsing-0.1.7 tensorflow-metadata-1.17.2 tensorflow_datasets-4.9.9\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7ce57f4-2a30-42b4-9d26-09d89318233d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 19:46:25.586398: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-16 19:46:25.586811: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 19:46:25.629375: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-16 19:46:26.681861: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-16 19:46:26.682516: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_imdb_texts():\n",
    "    ds_train, ds_test = tfds.load('imdb_reviews', split=['train','test'], as_supervised=True)\n",
    "    def ds_to_lists(ds):\n",
    "        texts, labels = [], []\n",
    "        for text, label in tfds.as_numpy(ds):\n",
    "            texts.append(text.decode('utf-8'))\n",
    "            labels.append(int(label))\n",
    "        return texts, labels\n",
    "    X_train, y_train = ds_to_lists(ds_train)\n",
    "    X_test, y_test = ds_to_lists(ds_test)\n",
    "    return X_train, y_train, X_test, y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9583ff47-6846-4042-8693-ea27ded85f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Using cached utils-1.0.2-py2.py3-none-any.whl\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for os\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install utils\n",
    "!pip install os\n",
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daf14089-87d9-426d-8853-00d9c72a21e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1763322405.353148     235 cuda_executor.cc:1309] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1763322405.384807     235 gpu_device.cc:2342] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-11-16 19:46:45.522691: I tensorflow/core/kernels/data/tf_record_dataset_op.cc:396] The default buffer size is 262144, which is overridden by the user specified `buffer_size` of 8388608\n",
      "2025-11-16 19:46:48.149574: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "2025-11-16 19:46:50.968250: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF pipeline saved to /tmp/models\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF + LogisticRegression training and saving\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from utils import *\n",
    "import os\n",
    "\n",
    "def main(output_dir=\"/tmp/models\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    X_train, y_train, X_test, y_test = load_imdb_texts()\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(stop_words=\"english\", max_features=50000)),\n",
    "        (\"clf\", LogisticRegression(max_iter=2000))\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Save pipeline\n",
    "    joblib.dump(pipeline, os.path.join(output_dir, \"tfidf_pipeline.joblib\"))\n",
    "    print(\"TF-IDF pipeline saved to\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4b065e-375e-4ef8-8ba0-b0642bc57cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 19:49:21.948218: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m45941/45941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2003s\u001b[0m 44ms/step - loss: 0.0033\n",
      "Epoch 2/3\n",
      "\u001b[1m 2797/45941\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30:34\u001b[0m 43ms/step - loss: 1.6531e-06"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m45941/45941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1983s\u001b[0m 43ms/step - loss: 1.0356e-06\n",
      "Epoch 3/3\n",
      "\u001b[1m45941/45941\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2038s\u001b[0m 44ms/step - loss: 5.1233e-07\n",
      "Word2Vec + classifier saved to /tmp/models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "from utils import *\n",
    "\n",
    "def train_word2vec_skipgram(tokenized_sequences, vocab_size, window_size=2, embedding_dim=128, epochs=5, batch_size=1024):\n",
    "    # Build dataset of (target, context, label) using Keras skipgrams helper per-sentence\n",
    "    pairs_target, pairs_context = [], []\n",
    "    for seq in tokenized_sequences:\n",
    "        sg_pairs, _ = skipgrams(sequence=seq, vocabulary_size=vocab_size, window_size=window_size, negative_samples=0.0)\n",
    "        for target, context in sg_pairs:\n",
    "            pairs_target.append(target)\n",
    "            pairs_context.append(context)\n",
    "    pairs_target = np.array(pairs_target, dtype='int32')\n",
    "    pairs_context = np.array(pairs_context, dtype='int32')\n",
    "\n",
    "    # Build model: two inputs (target, context) -> embeddings -> dot -> sigmoid\n",
    "    target_input = layers.Input(shape=(), dtype='int32')\n",
    "    context_input = layers.Input(shape=(), dtype='int32')\n",
    "\n",
    "    embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='w2v_embedding')\n",
    "    target_emb = embedding(target_input)\n",
    "    context_emb = embedding(context_input)\n",
    "\n",
    "    dot = layers.Dot(axes=-1)([target_emb, context_emb])  # shape (batch, dim) dot (batch, dim) -> scalar\n",
    "    dot = layers.Reshape((1,))(dot)\n",
    "    output = layers.Activation('sigmoid')(dot)\n",
    "\n",
    "    model = models.Model([target_input, context_input], output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizers.Adam(0.001))\n",
    "\n",
    "    # Prepare labels: since we used negative_samples=0, labels all 1 for these positive pairs.\n",
    "    labels = np.ones((len(pairs_target), 1), dtype='int32')\n",
    "\n",
    "    model.fit([pairs_target, pairs_context], labels, epochs=epochs, batch_size=batch_size)\n",
    "    return model\n",
    "\n",
    "def main(output_dir=\"/tmp/models\", embedding_dim=128):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    X_train, y_train, X_test, y_test = load_imdb_texts()\n",
    "\n",
    "    # Tokenize (fit on train)\n",
    "    tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size = min(len(tokenizer.word_index) + 1, 20000)  # cap vocab\n",
    "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "    X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "    X_test_seq  = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "    # Train Word2Vec (skip-gram style)\n",
    "    w2v_model = train_word2vec_skipgram(X_train_seq, vocab_size=vocab_size, window_size=4, embedding_dim=embedding_dim, epochs=3)\n",
    "\n",
    "    # Extract embedding weights\n",
    "    embedding_layer = w2v_model.get_layer('w2v_embedding')\n",
    "    embeddings = embedding_layer.get_weights()[0]  # shape (vocab_size, embedding_dim)\n",
    "\n",
    "    # Convert docs to vector by averaging token embeddings (use 0 for OOV/padding)\n",
    "    def doc_to_avg_vec(seqs):\n",
    "        doc_embs = []\n",
    "        for seq in seqs:\n",
    "            vecs = [embeddings[w] for w in seq if w < vocab_size and w>0]\n",
    "            if len(vecs) == 0:\n",
    "                doc_embs.append(np.zeros(embedding_dim, dtype=np.float32))\n",
    "            else:\n",
    "                doc_embs.append(np.mean(vecs, axis=0))\n",
    "        return np.stack(doc_embs)\n",
    "\n",
    "    X_train_emb = doc_to_avg_vec(X_train_seq)\n",
    "    X_test_emb  = doc_to_avg_vec(X_test_seq)\n",
    "\n",
    "    # Train simple classifier (LogisticRegression)\n",
    "    clf = LogisticRegression(max_iter=2000)\n",
    "    clf.fit(X_train_emb, y_train)\n",
    "\n",
    "    # Save tokenizer, embeddings, classifier\n",
    "    joblib.dump(tokenizer, os.path.join(output_dir, \"w2v_tokenizer.joblib\"))\n",
    "    np.save(os.path.join(output_dir, \"w2v_embeddings.npy\"), embeddings)\n",
    "    joblib.dump(clf, os.path.join(output_dir, \"w2v_clf.joblib\"))\n",
    "    print(\"Word2Vec + classifier saved to\", output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9df80cae-a967-4a53-ac65-053f883b5170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: I loved the movie — it was fantastic, great acting and story.\n",
      "TF-IDF -> {'label': 1, 'prob': [0.0023842351562546327, 0.9976157648437454]}\n",
      "Word2Vec -> {'label': 1, 'prob': [0.3650177620648217, 0.6349822379351783]}\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from utils import *  # optional for label names\n",
    "\n",
    "def load_tfidf_model(path=\"/tmp/models/tfidf_pipeline.joblib\"):\n",
    "    return joblib.load(path)\n",
    "\n",
    "def load_w2v_models(model_dir=\"/tmp/models\"):\n",
    "    tokenizer = joblib.load(os.path.join(model_dir, \"w2v_tokenizer.joblib\"))\n",
    "    embeddings = np.load(os.path.join(model_dir, \"w2v_embeddings.npy\"))\n",
    "    clf = joblib.load(os.path.join(model_dir, \"w2v_clf.joblib\"))\n",
    "    return tokenizer, embeddings, clf\n",
    "\n",
    "def predict_sample(sample_text, tfidf_pipeline, w2v_tokenizer, w2v_embeddings, w2v_clf):\n",
    "    # TF-IDF predict\n",
    "    tfidf_prob = tfidf_pipeline.predict_proba([sample_text])[0]\n",
    "    tfidf_label = tfidf_pipeline.predict([sample_text])[0]\n",
    "\n",
    "    # Word2Vec predict\n",
    "    tokens = w2v_tokenizer.texts_to_sequences([sample_text])[0]\n",
    "    vecs = [w2v_embeddings[w] for w in tokens if w>0 and w < len(w2v_embeddings)]\n",
    "    if len(vecs)==0:\n",
    "        doc_vec = np.zeros(w2v_embeddings.shape[1])\n",
    "    else:\n",
    "        doc_vec = np.mean(vecs, axis=0)\n",
    "    w2v_prob = w2v_clf.predict_proba([doc_vec])[0]\n",
    "    w2v_label = w2v_clf.predict([doc_vec])[0]\n",
    "\n",
    "    return {\n",
    "        \"tfidf\": {\"label\": int(tfidf_label), \"prob\": tfidf_prob.tolist()},\n",
    "        \"w2v\":  {\"label\": int(w2v_label), \"prob\": w2v_prob.tolist()}\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # sample\n",
    "    sample = \"I loved the movie — it was fantastic, great acting and story.\"\n",
    "    tfidf_pipeline = load_tfidf_model()\n",
    "    tokenizer, embeddings, clf = load_w2v_models()\n",
    "    preds = predict_sample(sample, tfidf_pipeline, tokenizer, embeddings, clf)\n",
    "    print(\"Sample:\", sample)\n",
    "    print(\"TF-IDF ->\", preds[\"tfidf\"])\n",
    "    print(\"Word2Vec ->\", preds[\"w2v\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede98314-a896-4c2b-9a74-5582845ec393",
   "metadata": {},
   "source": [
    "TF-IDF works extremely well when the sentence contains exact positive words it has seen during training:\n",
    "“loved”\n",
    "“fantastic”\n",
    "“great”\n",
    "“acting”\n",
    "“story”\n",
    "These are very common in positive movie reviews → TF-IDF gives a huge weight to these words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9c1e93-7c14-46a5-a82f-155f509fc1b8",
   "metadata": {},
   "source": [
    "Word2Vec embeddings are averaged before classification.\n",
    "Causes: Signal gets diluted\n",
    "Averaging word vectors →\n",
    "meaningful words like loved, fantastic, great\n",
    "get blended with less useful words like the, it, was.\n",
    "This reduces the classifier’s confidence.\n",
    "\n",
    "Word2Vec classifier may not be fully optimized\n",
    "Most common issues:\n",
    "embedding dimension too large/small\n",
    "classifier trained with fewer epochs\n",
    "no fine-tuning of embeddings\n",
    "logistic regression or simple dense layer trained on averaged vectors (weak model)\n",
    "Word2Vec trained on small corpus → weaker similarity structure\n",
    "So the prediction is correct (label = 1), but less confident (0.63)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a68c258a-aff7-4a5e-9e38-557cd61789e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: The visuals were gorgeous, but the story fell apart completely.\n",
      "TF-IDF -> {'label': 0, 'prob': [0.5100513130107627, 0.48994868698923727]}\n",
      "Word2Vec -> {'label': 1, 'prob': [0.36820853756602867, 0.6317914624339713]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # sample\n",
    "    sample = \"The visuals were gorgeous, but the story fell apart completely.\"\n",
    "    tfidf_pipeline = load_tfidf_model()\n",
    "    tokenizer, embeddings, clf = load_w2v_models()\n",
    "    preds = predict_sample(sample, tfidf_pipeline, tokenizer, embeddings, clf)\n",
    "    print(\"Sample:\", sample)\n",
    "    print(\"TF-IDF ->\", preds[\"tfidf\"])\n",
    "    print(\"Word2Vec ->\", preds[\"w2v\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4f796e1-5a9c-4a2d-83a5-c974242f31ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample: The film was a complete trainwreck.\n",
      "TF-IDF -> {'label': 0, 'prob': [0.6068276338599814, 0.39317236614001855]}\n",
      "Word2Vec -> {'label': 1, 'prob': [0.2417948268619774, 0.7582051731380226]}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # sample\n",
    "    sample = \"The film was a complete trainwreck.\"\n",
    "    tfidf_pipeline = load_tfidf_model()\n",
    "    tokenizer, embeddings, clf = load_w2v_models()\n",
    "    preds = predict_sample(sample, tfidf_pipeline, tokenizer, embeddings, clf)\n",
    "    print(\"Sample:\", sample)\n",
    "    print(\"TF-IDF ->\", preds[\"tfidf\"])\n",
    "    print(\"Word2Vec ->\", preds[\"w2v\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae3bd44-5608-4bcf-afa3-48705cb1efcc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
